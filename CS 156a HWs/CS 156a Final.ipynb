{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"14aszlPeh18xKnD-i0p5IWxa5IqIQYCbb","authorship_tag":"ABX9TyO///hg0SRihPdddYl/Xgqx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Imports\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import svm\n","from sklearn import cluster"],"metadata":{"id":"Pk5Zax_QusHJ","executionInfo":{"status":"ok","timestamp":1701219339018,"user_tz":480,"elapsed":135,"user":{"displayName":"Alfin Hou","userId":"02529736084419145803"}}},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":["## Question 1##\n","We just need to consider the nunmber of possible $x_{1}^mx_{2}^{n}$ such that $m + n \\leq 10$ and $m,n$ are not $0$ at the same time.  \n","The total number of this posibility is $(11 + 1)\\times 11 \\times \\frac{1}{2} - 1 = 65$.\n","\n","Therefore, the answer to question 1 is **[e]**."],"metadata":{"id":"MCNscAS8uLf1"}},{"cell_type":"markdown","source":["##Question 2##\n","Note that the hypothesis of logistic regression model takes the form of $h(x) = \\frac{e^s}{1+e^s}$ where $s$ is a function fo $x$.\n","In this case, if $g^{(1)}(x) = \\frac{e^{s_1}}{1+e^{s_1}}$ and $g^{(2)}(x) = \\frac{e^{s_2}}{1+e^{s_2}}$, we can easily notice that $\\bar{g}(x) = \\frac{e^{s_1}+e^{s_2} + 2e^{s_1+s_2}}{2(1 + e^{s_1} +e^{s_2} + e^{s_1+s_2})} \\notin \\mathcal{H}$.\n","\n","Therefore, the answer to question 2 is **[d]**."],"metadata":{"id":"X9XuwIsoxny5"}},{"cell_type":"markdown","source":["##Question 3##\n","Consider the following senario: we have two sample points $(1,0),(-1,0)$, and the target function is $y = 0$.  \n","Now if we consider the following hypotheses: $h_1(x) = 0$, $h_2(x) = x^2-1$. We will notice that both functions fit the points perfectly, meaning $E_{in}=0$ for both $h_1$ and $h_2$. But cleary $h_2$ is overfitting and its $E_{out}$ will be huge if we sample more points from $y=0$ to test.  \n","Thus, we have demonstrated a case when there is no difference in $E_{in}$ but there is an overfitting.\n","\n","Therefore, the answer to question 3 is **[a]**."],"metadata":{"id":"bGSxN5vtz999"}},{"cell_type":"markdown","source":["##Question 4##\n","One main difference between stochastic noise and deterministic noice is that deterministic noice depends on the hypothesis set $\\mathcal{H}$ but stochastic noise doesn't (see lecture slides 11). This is because stochastic noise can't be captured by the hypotheses (It's the noise level $\\epsilon(x) = \\sigma^2$).\n","\n","Therefore, the answer to question 4 is **[d]**."],"metadata":{"id":"mRwKUcdp1vj8"}},{"cell_type":"markdown","source":["##Question 5 - 6##\n","For question 5, since the linear regression solution, which solves the minimization problem in an unconstrainted setting, already satisfied the constraints, we can conclude that $w_{reg} = w_{lin}$ because it $w_{lin}$ will also solve the constrained (regularized) minization problem.  \n","For question 6, we have the following equivalence:\n","$$\n","\\text{Minimize }E_{aug} = \\frac{1}{N}(Zw-y)^T(Zw-y) + \\frac{\\lambda}{N}w^Tw\n","$$\n","is equivalent to:\n","$$\n","\\text{Minimize }\\frac{1}{N}(Zw-y)^T(Zw-y)\\text{ subject to } w^Tw\\leq C\n","$$\n","Hence, we can say that the augmented error captures the soft constraint using an additional term $\\frac{\\lambda}{N}w^Tw$. (See lecture slides 12).\n","\n","Therefore, the answer to question 5 and 6 are:\n","5.  **[a]**\n","6.  **[b]**"],"metadata":{"id":"rbgx2rTl3XCR"}},{"cell_type":"markdown","source":["##Question 7 - 10##\n"],"metadata":{"id":"3tdKxOhF1Wht"}},{"cell_type":"code","execution_count":55,"metadata":{"id":"tTA06hE_uJdH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701219339387,"user_tz":480,"elapsed":173,"user":{"displayName":"Alfin Hou","userId":"02529736084419145803"}},"outputId":"a37d589c-5f78-41b3-f03c-e1038937226b"},"outputs":[{"output_type":"stream","name":"stdout","text":["0  versus all:\n","E_in with no transform: 0.10931285146070498 E_out with no transform: 0.11509715994020926\n","E_in with transform: 0.10231792621039638 E_out with transform: 0.10662680617837568\n"," \n","1  versus all:\n","E_in with no transform: 0.01522424907420107 E_out with no transform: 0.02242152466367713\n","E_in with transform: 0.012343985735838706 E_out with transform: 0.02192326856003986\n"," \n","2  versus all:\n","E_in with no transform: 0.10026059525442327 E_out with no transform: 0.09865470852017937\n","E_in with transform: 0.10026059525442327 E_out with transform: 0.09865470852017937\n"," \n","3  versus all:\n","E_in with no transform: 0.09024825126868742 E_out with no transform: 0.08271051320378675\n","E_in with transform: 0.09024825126868742 E_out with transform: 0.08271051320378675\n"," \n","4  versus all:\n","E_in with no transform: 0.08942531888629818 E_out with no transform: 0.09965122072745392\n","E_in with transform: 0.08942531888629818 E_out with transform: 0.09965122072745392\n"," \n","5  versus all:\n","E_in with no transform: 0.07625840076807022 E_out with no transform: 0.07972097658196313\n","E_in with transform: 0.07625840076807022 E_out with transform: 0.07922272047832586\n"," \n","6  versus all:\n","E_in with no transform: 0.09107118365107666 E_out with no transform: 0.08470353761833582\n","E_in with transform: 0.09107118365107666 E_out with transform: 0.08470353761833582\n"," \n","7  versus all:\n","E_in with no transform: 0.08846523110684405 E_out with no transform: 0.07324364723467862\n","E_in with transform: 0.08846523110684405 E_out with transform: 0.07324364723467862\n"," \n","8  versus all:\n","E_in with no transform: 0.07433822520916199 E_out with no transform: 0.08271051320378675\n","E_in with transform: 0.07433822520916199 E_out with transform: 0.08271051320378675\n"," \n","9  versus all:\n","E_in with no transform: 0.08832807570977919 E_out with no transform: 0.08819133034379671\n","E_in with transform: 0.08832807570977919 E_out with transform: 0.08819133034379671\n"," \n","For 1 versus 5:\n","Lambda =  0.01\n","E_in with transform: 0.004484304932735426 E_out with transform: 0.02830188679245283\n"," \n","Lambda =  1\n","E_in with transform: 0.005124919923126201 E_out with transform: 0.025943396226415096\n"," \n"]}],"source":["# Load datasets\n","D_train = np.loadtxt(\"/content/drive/MyDrive/Colab Notebooks/CS 156a HWs/features.train.txt\", dtype = np.float64)\n","D_test = np.loadtxt(\"/content/drive/MyDrive/Colab Notebooks/CS 156a HWs/features.test.txt\")\n","D_train_const = np.c_[np.ones(len(D_train)), D_train[:,1], D_train[:,2]]\n","D_test_const = np.c_[np.ones(len(D_test)), D_test[:,1], D_test[:,2]]\n","\n","digits = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n","X_mat = D_train_const\n","Z_mat = np.c_[np.ones(len(D_train)), D_train[:,1], D_train[:,2], D_train[:,1] * D_train[:,2], D_train[:,1] ** 2, D_train[:,2] ** 2]\n","D_test_Z = np.c_[np.ones(len(D_test)), D_test[:,1], D_test[:,2], D_test[:,1] * D_test[:,2], D_test[:,1] ** 2, D_test[:,2] ** 2]\n","for n in digits:\n","  lam = 1\n","  y_train = np.array([1 if i == n else -1 for i in D_train[:,0]])\n","  y_test = np.array([1 if i == n else -1 for i in D_test[:,0]])\n","  XTX = np.matmul(X_mat.T, X_mat)\n","  ZTZ = np.matmul(Z_mat.T, Z_mat)\n","  w_reg_X = np.dot(np.matmul(np.linalg.inv(XTX + lam * np.identity(len(XTX))),X_mat.T), y_train)\n","  w_reg_Z = np.dot(np.matmul(np.linalg.inv(ZTZ + lam * np.identity(len(ZTZ))),Z_mat.T), y_train)\n","  E_in_x = np.mean(np.not_equal(np.sign(np.matmul(X_mat, w_reg_X)), y_train))\n","  E_out_x = np.mean(np.not_equal(np.sign(np.matmul(D_test_const, w_reg_X)), y_test))\n","  E_in_z = np.mean(np.not_equal(np.sign(np.matmul(Z_mat, w_reg_Z)), y_train))\n","  E_out_z = np.mean(np.not_equal(np.sign(np.matmul(D_test_Z, w_reg_Z)), y_test))\n","  print(n,\" versus all:\")\n","  print(\"E_in with no transform:\", E_in_x,\"E_out with no transform:\",E_out_x)\n","  print(\"E_in with transform:\", E_in_z,\"E_out with transform:\",E_out_z)\n","  print(\" \")\n","\n","# 1 vs 5\n","print(\"For 1 versus 5:\")\n","D_train_ovo = D_train_const[(D_train[:,0] == 1) | (D_train[:,0] == 5)]\n","D_test_ovo = D_test_const[(D_test[:,0] == 1) | (D_test[:,0] == 5)]\n","y_train_ovo = np.array([1 if i == 1 else -1 for i in D_train[:,0][(D_train[:,0] == 1) | (D_train[:,0] == 5)]])\n","y_test_ovo = np.array([1 if i == 1 else -1 for i in D_test[:,0][(D_test[:,0] == 1) | (D_test[:,0] == 5)]])\n","Z_ovo = np.c_[np.ones(len(D_train_ovo)), D_train_ovo[:,1], D_train_ovo[:,2], D_train_ovo[:,1] * D_train_ovo[:,2], D_train_ovo[:,1] ** 2, D_train_ovo[:,2] ** 2]\n","D_test_ovo_Z = np.c_[np.ones(len(D_test_ovo)), D_test_ovo[:,1], D_test_ovo[:,2], D_test_ovo[:,1] * D_test_ovo[:,2], D_test_ovo[:,1] ** 2, D_test_ovo[:,2] ** 2]\n","for lam in [0.01, 1]:\n","  ZTZ_ovo = np.matmul(Z_ovo.T, Z_ovo)\n","  w_ovo_Z = np.dot(np.matmul(np.linalg.inv(ZTZ_ovo + lam * np.identity(len(ZTZ_ovo))),Z_ovo.T), y_train_ovo)\n","  E_in_z_ovo = np.mean(np.not_equal(np.sign(np.matmul(Z_ovo, w_ovo_Z)), y_train_ovo))\n","  E_out_z_ovo = np.mean(np.not_equal(np.sign(np.matmul(D_test_ovo_Z, w_ovo_Z)), y_test_ovo))\n","  print(\"Lambda = \", lam)\n","  print(\"E_in with transform:\", E_in_z_ovo,\"E_out with transform:\",E_out_z_ovo)\n","  print(\" \")"]},{"cell_type":"markdown","source":["From the simiulations above, we can conclude that the answers to question 7 - 10 are:\n","7.  **[d]**\n","8.  **[b]**\n","9.  **[e]**\n","10.  **[a]**"],"metadata":{"id":"5f7Si0Yc4vF9"}},{"cell_type":"markdown","source":["## Question 11 - 12 ##"],"metadata":{"id":"uvsQL7DBu-2p"}},{"cell_type":"code","source":["# Define the set and transform\n","X = np.array([\n","    [1,0],\n","    [0,1],\n","    [0,-1],\n","    [-1,0],\n","    [0,2],\n","    [0,-2],\n","    [-2,0]])\n","y = np.array([-1,-1,-1,1,1,1,1])\n","Z = np.c_[X[:,1] ** 2 - 2 * X[:,0] - np.ones(7), X[:,0] ** 2 - 2 * X[:,1] + np.ones(7)]\n","plt.scatter(Z[:,0], Z[:,1], c = y)\n","plt.axvline(x=0.5, color='r', linestyle='-')\n","plt.axvline(x=0, color='b', linestyle='--')\n","plt.axvline(x=1, color='b', linestyle='--')\n","plt.xlabel(\"$x_1$\")\n","plt.ylabel(\"$x_2$\")\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":450},"id":"Vz44YUjIvQN8","executionInfo":{"status":"ok","timestamp":1701219339974,"user_tz":480,"elapsed":590,"user":{"displayName":"Alfin Hou","userId":"02529736084419145803"}},"outputId":"bd81107d-e18f-4680-dff2-154487885c3e"},"execution_count":56,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["From the plot above, we can conclude that maximize the margin is $x_1 = 0.5$, which is equivalent to:\n","$$\n","\\begin{bmatrix}\n","1 & 0\n","\\end{bmatrix}\n","\\begin{bmatrix}\n","x_1 \\\\\n","x_2\n","\\end{bmatrix}\n","-0.5=0\n","$$\n","Hence, we have $w^T=[1\\quad 0]$ and $b = -0.5$.\n","\n","Therefore, the answer to question 11 is **[c]**."],"metadata":{"id":"6v8o8cRFarsZ"}},{"cell_type":"code","source":["# Use a very large C to mimic hard-margin SVM\n","hard_margin = svm.SVC(C = 1e20, kernel = 'poly', degree = 2, coef0 = 1, gamma = 1)\n","hard_margin.fit(X, y)\n","print(\"Number of support vectors:\", len(hard_margin.support_vectors_))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8uhpGWf5cRwl","executionInfo":{"status":"ok","timestamp":1701219339975,"user_tz":480,"elapsed":7,"user":{"displayName":"Alfin Hou","userId":"02529736084419145803"}},"outputId":"4ebd0c7a-6a5c-4c0f-a5f1-135ad48cc99f"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of support vectors: 5\n"]}]},{"cell_type":"markdown","source":["From the simulation above, the answer to question 12 is **[c]**."],"metadata":{"id":"VJ6ErUPJd0UR"}},{"cell_type":"markdown","source":["##Question 13 - 18##\n"],"metadata":{"id":"VwEvxonXd44t"}},{"cell_type":"code","source":["gam = 1.5\n","N = 100\n","pi = np.pi\n","\n","# Kernel form:\n","non_separable = 0\n","for run in range(10001):\n","  # Sampling\n","  samples = np.random.uniform(-1, 1, (N, 2))\n","  X = np.c_[np.ones(N), samples[:,0], samples[:,1]]\n","  y = np.sign(samples[:,1] - samples[:,0] + 0.25 * np.sin(pi * samples[:,0]))\n","  rbf_svc = svm.SVC(C = 1e40, kernel = 'rbf', gamma = gam)\n","  rbf_svc.fit(X, y)\n","  y_pred = rbf_svc.predict(X)\n","  if not(np.all(y_pred == y)):\n","    non_separable += 1\n","  if run in [2000, 4000, 6000, 8000, 10000]:\n","    print(\"Non-separable percentage = \", non_separable/run)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nIT-noXSd9AI","executionInfo":{"status":"ok","timestamp":1701219354193,"user_tz":480,"elapsed":14223,"user":{"displayName":"Alfin Hou","userId":"02529736084419145803"}},"outputId":"63dae0ae-f21d-4002-9e02-249767f7dadf"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["Non-separable percentage =  0.0\n","Non-separable percentage =  0.0\n","Non-separable percentage =  0.0\n","Non-separable percentage =  0.0\n","Non-separable percentage =  0.0\n"]}]},{"cell_type":"markdown","source":["From the simulation, the answer to question 13 is **[a]**."],"metadata":{"id":"drIcgMsQi0V3"}},{"cell_type":"code","source":["K_list = [9, 12]\n","gam_list = [1.5]\n","for gam in gam_list:\n","  for K in K_list:\n","    out_performed = 0\n","    for run in range(10000):\n","      # Sampling\n","      samples = np.random.uniform(-1, 1, (N, 2))\n","      X = np.c_[np.ones(N), samples[:,0], samples[:,1]]\n","      y = np.sign(samples[:,1] - samples[:,0] + 0.25 * np.sin(pi * samples[:,0]))\n","      test = np.random.uniform(-1, 1, (5*N, 2))\n","      X_test = np.c_[np.ones(5*N), test[:,0], test[:,1]]\n","      y_test = np.sign(test[:,1] - test[:,0] + 0.25 * np.sin(pi * test[:,0]))\n","      # Kernel\n","      rbf_svc = svm.SVC(C = 1e40, kernel = 'rbf', gamma = gam)\n","      rbf_svc.fit(X, y)\n","      y_pred = rbf_svc.predict(X_test)\n","      E_out_ker = np.mean(np.not_equal(y_pred, y_test))\n","      # Regular\n","      kMeans = cluster.KMeans(n_clusters = K, init = 'random', n_init = 1)\n","      centres = kMeans.fit(X).cluster_centers_\n","      phi = np.ones((N,K+1))\n","      for col in range(0, K):\n","        phi[:,col+1] = np.exp(-gam * np.linalg.norm(X - centres[col], axis = 1) ** 2)\n","      w = np.dot(np.matmul(np.linalg.inv(np.matmul(phi.T, phi)),phi.T),y)\n","      phi_test = np.ones((5*N, K+1))\n","      for col in range(0, K):\n","        phi_test[:,col+1] = np.exp(-gam * np.linalg.norm(X_test - centres[col], axis = 1) ** 2)\n","      y_reg = np.sign(np.matmul(phi_test, w))\n","      E_out_reg = np.mean(np.not_equal(y_reg, y_test))\n","      if E_out_ker < E_out_reg:\n","        out_performed += 1\n","      if run in [3000,6000,9000]:\n","        print(run)\n","    print(\"Gamma = \",gam,\"K = \",K,\"Percentage kernel beats regular = \",out_performed/10000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Su5pHhx7jEaU","executionInfo":{"status":"ok","timestamp":1701219739977,"user_tz":480,"elapsed":385791,"user":{"displayName":"Alfin Hou","userId":"02529736084419145803"}},"outputId":"13838fb1-f024-4dd9-e07b-d6ef44a82390"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["3000\n","6000\n","9000\n","Gamma =  1.5 K =  9 Percentage kernel beats regular =  0.8852\n","3000\n","6000\n","9000\n","Gamma =  1.5 K =  12 Percentage kernel beats regular =  0.7836\n"]}]},{"cell_type":"markdown","source":["From the simulations, the answers to question 13 - 15 are:\n","13.  **[a]**\n","14.  **[e]**\n","15.  **[d]**"],"metadata":{"id":"jaCd5o5429ME"}},{"cell_type":"code","source":["# Now we need to compare for the same set of data\n","K_list = [9, 12]\n","gam = 1.5\n","situation = np.zeros(5)\n","E_in_list = np.zeros((10000,2))\n","E_out_list = np.zeros((10000,2))\n","for run in range(10000):\n","  samples = np.random.uniform(-1, 1, (N, 2))\n","  X = np.c_[np.ones(N), samples[:,0], samples[:,1]]\n","  y = np.sign(samples[:,1] - samples[:,0] + 0.25 * np.sin(pi * samples[:,0]))\n","  test = np.random.uniform(-1, 1, (5*N, 2))\n","  X_test = np.c_[np.ones(5*N), test[:,0], test[:,1]]\n","  y_test = np.sign(test[:,1] - test[:,0] + 0.25 * np.sin(pi * test[:,0]))\n","  for k in range(2):\n","    K = K_list[k]\n","    kMeans = cluster.KMeans(n_clusters = K, init = 'random', n_init = 1)\n","    centres = kMeans.fit(X).cluster_centers_\n","    phi = np.ones((N,K+1))\n","    for col in range(0, K):\n","      phi[:,col+1] = np.exp(-gam * np.linalg.norm(X - centres[col], axis = 1) ** 2)\n","    w = np.dot(np.matmul(np.linalg.inv(np.matmul(phi.T, phi)),phi.T),y)\n","    phi_test = np.ones((5*N, K+1))\n","    for col in range(0, K):\n","      phi_test[:,col+1] = np.exp(-gam * np.linalg.norm(X_test - centres[col], axis = 1) ** 2)\n","    y_reg_in = np.sign(np.matmul(phi, w))\n","    y_reg = np.sign(np.matmul(phi_test, w))\n","    E_in_reg = np.mean(np.not_equal(y_reg_in, y))\n","    E_out_reg = np.mean(np.not_equal(y_reg, y_test))\n","    E_in_list[run][k] = E_in_reg\n","    E_out_list[run][k] = E_out_reg\n","\n","E_in_compare = E_in_list[:,1] - E_in_list[:,0]\n","E_out_compare = E_out_list[:,1] - E_out_list[:,0]\n","for i in range(10000):\n","  if E_in_compare[i] < 0  and E_out_compare[i] > 0:\n","    situation[0] +=1\n","  elif E_in_compare[i] > 0  and E_out_compare[i] < 0:\n","    situation[1] +=1\n","  elif E_in_compare[i] > 0  and E_out_compare[i] > 0:\n","    situation[2] +=1\n","  elif E_in_compare[i] < 0  and E_out_compare[i] < 0:\n","    situation[3] +=1\n","  else:\n","    situation[4] +=1\n","\n","result = np.argmax(situation)\n","print(\"K from 9 to 12:\")\n","if result == 0:\n","  print(\"E_in goes down, but E_out goes up\")\n","elif result == 1:\n","  print(\"E_in goes up, but E_out goes down\")\n","elif result == 2:\n","  print(\"Both E_in and E_out go up\")\n","elif result == 3:\n","  print(\"Both E_in and E_out go down\")\n","else:\n","  print(\"Remain the same\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"49031RbD62aB","executionInfo":{"status":"ok","timestamp":1701220017627,"user_tz":480,"elapsed":277677,"user":{"displayName":"Alfin Hou","userId":"02529736084419145803"}},"outputId":"108972a8-6d85-4790-fbbb-f8d986ad3129"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["K from 9 to 12:\n","Both E_in and E_out go down\n"]}]},{"cell_type":"markdown","source":["From the simulation above, the answer to question 16 is **[d]**.\n"],"metadata":{"id":"wWYS_5YXGkOv"}},{"cell_type":"code","source":["K = 9\n","gam_list = [1.5, 2]\n","situation = np.zeros(5)\n","E_in_list = np.zeros((10000,2))\n","E_out_list = np.zeros((10000,2))\n","for run in range(10000):\n","  samples = np.random.uniform(-1, 1, (N, 2))\n","  X = np.c_[np.ones(N), samples[:,0], samples[:,1]]\n","  y = np.sign(samples[:,1] - samples[:,0] + 0.25 * np.sin(pi * samples[:,0]))\n","  test = np.random.uniform(-1, 1, (5*N, 2))\n","  X_test = np.c_[np.ones(5*N), test[:,0], test[:,1]]\n","  y_test = np.sign(test[:,1] - test[:,0] + 0.25 * np.sin(pi * test[:,0]))\n","  for g in range(2):\n","    gam = gam_list[k]\n","    kMeans = cluster.KMeans(n_clusters = K, init = 'random', n_init = 1)\n","    centres = kMeans.fit(X).cluster_centers_\n","    phi = np.ones((N,K+1))\n","    for col in range(0, K):\n","      phi[:,col+1] = np.exp(-gam * np.linalg.norm(X - centres[col], axis = 1) ** 2)\n","    w = np.dot(np.matmul(np.linalg.inv(np.matmul(phi.T, phi)),phi.T),y)\n","    phi_test = np.ones((5*N, K+1))\n","    for col in range(0, K):\n","      phi_test[:,col+1] = np.exp(-gam * np.linalg.norm(X_test - centres[col], axis = 1) ** 2)\n","    y_reg_in = np.sign(np.matmul(phi, w))\n","    y_reg = np.sign(np.matmul(phi_test, w))\n","    E_in_reg = np.mean(np.not_equal(y_reg_in, y))\n","    E_out_reg = np.mean(np.not_equal(y_reg, y_test))\n","    E_in_list[run][g] = E_in_reg\n","    E_out_list[run][g] = E_out_reg\n","\n","E_in_compare = E_in_list[:,1] - E_in_list[:,0]\n","E_out_compare = E_out_list[:,1] - E_out_list[:,0]\n","for i in range(10000):\n","  if E_in_compare[i] < 0  and E_out_compare[i] > 0:\n","    situation[0] +=1\n","  elif E_in_compare[i] > 0  and E_out_compare[i] < 0:\n","    situation[1] +=1\n","  elif E_in_compare[i] > 0  and E_out_compare[i] > 0:\n","    situation[2] +=1\n","  elif E_in_compare[i] < 0  and E_out_compare[i] < 0:\n","    situation[3] +=1\n","  else:\n","    situation[4] +=1\n","\n","result = np.argmax(situation)\n","print(\"Gamma from 1.5 to 2\")\n","if result == 0:\n","  print(\"E_in goes down, but E_out goes up\")\n","elif result == 1:\n","  print(\"E_in goes up, but E_out goes down\")\n","elif result == 2:\n","  print(\"Both E_in and E_out go up\")\n","elif result == 3:\n","  print(\"Both Ein and Eout go down\")\n","else:\n","  print(\"Remain the same\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"giAUmB-2G2SP","executionInfo":{"status":"ok","timestamp":1701220293768,"user_tz":480,"elapsed":276148,"user":{"displayName":"Alfin Hou","userId":"02529736084419145803"}},"outputId":"a63883d5-bdb1-42fe-dbb6-57afb26d7958"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["Gamma from 1.5 to 2\n","Both E_in and E_out go up\n"]}]},{"cell_type":"markdown","source":["Therefore, from the simulation, the answer to question 17 is **[c]**."],"metadata":{"id":"spyaIqf1LDqw"}},{"cell_type":"code","source":["print(\"Percentage of time when E_in = 0:\", np.sum(E_in_list[:,0] == 0)/10000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RWNWuB67LMO6","executionInfo":{"status":"ok","timestamp":1701220293768,"user_tz":480,"elapsed":11,"user":{"displayName":"Alfin Hou","userId":"02529736084419145803"}},"outputId":"4016e370-9c49-486a-8bd2-33f660c28c23"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["Percentage of time when E_in = 0: 0.024\n"]}]},{"cell_type":"markdown","source":["Therefore, from the simulation, the answer to question 18 is **[a]**."],"metadata":{"id":"VmwWXOPgNHU6"}},{"cell_type":"markdown","source":["##Question 19##\n","\n","Since the sample point had a heart attack, we know that higher $h$ will have higher likelihood. Thus, the posterior should be increasing.  \n","Also, Since we know the following formula:\n","$$\n","P(h = f|D) \\propto P(h = f)P(D|h = f)\n","$$\n","We know that the increase should be linear.\n","\n","Therefore, the answer to question 19 is **[b]**."],"metadata":{"id":"4FjxHdVKNSFD"}},{"cell_type":"markdown","source":["##Question 20##\n","The errors of $g_1$ and $g_2$ are given by:\n","For any $x\\in X$, the squared error for $g_1$ and $g_2$ are:\n","$$e_1 = (g_1(x)-f(x))^2, e_2 = (g_2(x)-f(x))^2$$\n","And for $g(x) = \\frac{1}{2}(g_1(x)+g_2(x))$, we have:\n","$$e_g = \\frac{1}{4}(e_1 + e_2) + \\frac{1}{2}(g_1(x)-f(x))(g_2(x)-f(x)) = \\frac{1}{4}(e_1 + e_2) + \\frac{1}{2}\\sqrt{e_1e_2}$$\n","By AM-GM inequality, we have $\\frac{1}{2}(e_1 + e_2) \\geq \\sqrt{e_1e_2}$\n","Hence, we have the following:\n","$$\n","e_g = \\frac{1}{4}(e_1 + e_2) + \\frac{1}{2}\\sqrt{e_1e_2} \\leq \\frac{1}{4}(e_1 + e_2) + \\frac{1}{4}(e_1 + e_2) = \\frac{1}{2}(e_1 + e_2)\n","$$\n","Thus, for any $x\\in X$, $e_g$ is smaller than the average of $e_1$ and $e_2$. Hence, we have that $E_{out}(g)$ can't be worse than the average of $E_{out}(g_1)$ and $E_{out}(g_2)$.\n","\n","Therefore, the answer to question 20 is **[c]**.\n","\n"],"metadata":{"id":"Ea9V1MHiTo8L"}}]}