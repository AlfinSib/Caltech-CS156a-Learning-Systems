{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPjrmYPlCCJNa3wWEsEKr4i"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Question 1#\n","(i) is not learning because the pattern is not learnt from data. Instead, the pattern is derived from the specification that is provided. Hence, no Machine Learning is present.\n","\n","(ii) is Supervised Learning because:\n","- It's Machine Learning because it is learning the pattern from data (i.e. labeled coins).\n","- It's supervised because the data given is in the form of (input, correct output) as the the set of coins data is labeled.\n","\n","(iii) is reinforcement learning because:\n","\n","- It's Machine Learning because it also learns the pattern from data. In this case, it's the historical results played by itself.\n","- It's reinforcement learning because the output is given in the form of game results. And although there isn't any explicit indication of whether an output is correct, a \"grade\" is given by penalizing bad or losing moves.\n","\n","Hence, I choose **d** for this question.\n","\n","##Question 2##\n","(ii) is best suited because it has the 3 essences of Machine Learning.\n","\n","- It's expected that a pattern exists that relates potential fraud and credit card charges (e.g. charges that involves a large amount of money and is international is more likely to be related to frauds).\n","- It has data, because each charge in the past is recorded and whether it's related to a fraud is also recorded and available (i.e. from card user report).\n","- It's mathematically complicated to derive an explicit formula that describes the pattern since many other factors are involved in an event of fraud.\n","\n","(iv) is also best suited because it also has the 3 essences of Machine Learning.\n","- It's also expected to have a pattern because traffic lights can significantly influence  the speed of cars (e.g. stops at red light).\n","- Historical traffic data such as the number of cars passing the intersection is very likely to be available.\n","- It's also hard to have explicit mathematical expressions due to too many parameters involved (e.g. Number of cars in the intersection, traffic light combinations etc.).\n","\n","(i) is not best suited because we can pin this problem down mathematically in an accurate way: Check if it's divisible by any integer in $[2, n/2]$.\n","\n","(iii) is also not best suited because we can pin down this problem mathematically by using physical formulas. Even if there are errors due to factors such as air resistance force, we can still expect to get an good enough approximation, which is likely to be better or as good as a Machine Learning approximation.\n","\n","Hence, I choose **a** for this question.\n","\n","##Question 3##\n","Let $A, B$ denote the following events:\n","- $A$: The second ball is black\n","- $B$: The first ball is black\n","\n","We need to calculate $\\mathbb{P}(A|B)$.\n","\n","By the definition of conditional probability, we have the following:\n","$$\\mathbb{P}(A|B)=\\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(B)}$$\n","\n","Here, $\\mathbb{P}(B)=\\frac{1}{2} \\times 1 + \\frac{1}{2} \\times \\frac{1}{2}=\\frac{3}{4}$ because in order for the first ball to be black, we either choose the bag with a white ball and take out the black ball, which has a probability of $\\frac{1}{2}$, or we choose the other bag and take out any ball in the bag. And we choose the bag randomly, it means any bag has a probability of $\\frac{1}{2}$ to be chosen. Hence, we get the above equation for $\\mathbb{P}(B)$.\n","\n","Also, $\\mathbb{P}(A\\cap B) = \\frac{1}{2} \\times 0 + \\frac{1}{2}\\times 1 = \\frac{1}{2}$. This is because the event $A\\cap B$ is just the event that you draw $2$ black balls. This is impossible if you choose the bag with only $1$ black ball, and is certain if you choose the bag with $2$ black balls. And since you choose the bag randomly, meaning any bag has a probability of $\\frac{1}{2}$ to be chosen, we can derive the equation above for $\\mathbb{P}(A\\cap B)$.\n","\n","Hence, the solution will be as follows:\n","\n","$$\\mathbb{P}(A|B)=\\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(B)} = \\frac{1/2}{3/4}=\\frac{2}{3}$$\n","\n","Hence, I choose **d** for this question.\n","\n","##Question 4##\n","For each marble drawn, you either draw a red marble, or you draw a non-red marble. That is:\n","$$\\mathbb{P}(Red) + \\mathbb{P}(Non-red) = 1$$\n","Thus, the probability of getting no red marble in one draw, is just the probability for you to get a non-red marble, which is given by:\n","$$\\mathbb{P}(Non-red) = 1 - \\mathbb{P}(Red) = 1 - 0.55 = 0.45$$\n","Thus, for this to occur $10$ times, the corresponding probability is just $0.45^{10}\\approx3.405\\times 10^{-4}$.\n","\n","Hence, I choose **b** for this question.\n","\n","##Question 5##\n","Since for each sample, you either get $v=0$ or you don't (there is no other possible event), you either have no sample with $v=0$, or you have at least $1$ sample with $v=0$. The probability for the event \"no sample with $v=0$\", denoted by $p$, is:\n","$$p = (1 - 0.45^{10})^{1000} \\approx 0.7113$$\n","Hence, the probability for at least $1$ sample to have $v=0$, is just $1-p \\approx 1 - 0.7113 \\approx 0.2887 \\approx 0.289$.\n","\n","Hence, I choose **c** for this question.\n","\n","##Question 6##\n","For (i), the number of functions that agrees with all inputs is just $1$, since it needs to return $1$ for all $3$ inputs. And the number of functions that agrees with $2$ out of $3$ inputs, is $3$, as it is equivalent to choosing exactly $1$ point (out of $3$) that doesn't return $1$. Lastly, the number of functions that agrees with $1$ out of $3$ inputs, is also $3$ because it's equivalent to choosing $1$ out of $3$ points and return $1$ for that input. We don't need to consider the situation of $0$ agreeing point because it doesn't contribute to the score. Hence, the score for (i) is:\n","$$1 \\times 3 + 3 \\times 2 + 3 \\times 1 = 12$$  \n","For (ii), the score can be calculated in the same way as (i), hence, the its score is also:\n","$$1 \\times 3 + 3 \\times 2 + 3 \\times 1 = 12$$\n","For (iii), $g(101) = g(110) = 0, g(111) = 1$. Though the values returned by $g$ is different, we can also consider it to be in the same situation as (i) and (ii) because the (input, output) combinations are fixed. So the number of functions that agrees with all $3$ points is also $1$ since it returns exactly the same output for each input. Also, the number of functions that agrees with $2$ points is $3$ as we just need to choose $1$ point that returns a different output, and we have $3$ possible choices. Lastly, the number of functions that agrees with $1$ out of $3$ points, is $3$ because we just need to choose $1$ point and let it return the same output as $g$ and let all the other points return a different output. Hence, the score is as follows:\n","$$1 \\times 3 + 3 \\times 2 + 3 \\times 1 = 12$$  \n","For (iv), its score can be calculated using the same logic as $3$ and since the situation is basically the same, the score will be the same as (iii), which is\n","$$1 \\times 3 + 3 \\times 2 + 3 \\times 1 = 12$$\n","Therefore, as we can see, all $4$ functions give the same score.  \n","Hence, I choose **e** for this question.\n"],"metadata":{"id":"HdZwahlj7SMV"}},{"cell_type":"markdown","source":["##Question 7-10##\n","For the next 4 questions, the following code is used to generate simulation results related to them.\n"],"metadata":{"id":"MM5TuAnHMH7X"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PIrBlv6f7OMG","executionInfo":{"status":"ok","timestamp":1695948803025,"user_tz":420,"elapsed":14048,"user":{"displayName":"Alfin Hou","userId":"02529736084419145803"}},"outputId":"3c7d015c-8582-47e3-d004-9dc9e28b2c3c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Current N value: 10\n","Mean number of iterations: 10.529\n","Mean disagreement probability: 0.10596549999999999\n","Current N value: 100\n","Mean number of iterations: 101.377\n","Mean disagreement probability: 0.0136042\n"]}],"source":["# Imports\n","import numpy as np\n","\n","# Initialize variables\n","N_choices = [10,100]\n","\n","# Experiment (1000 runs)\n","for N in N_choices:\n","  # Prepare to calculate mean iterations and probability\n","  num_iter = []\n","  probs = []\n","  print(\"Current N value:\", N)\n","  for _ in range(1000):\n","    iter = 0\n","    # Generate lines and samples\n","    point_head = np.random.uniform(-1, 1, 2)\n","    point_tail = np.random.uniform(-1, 1, 2)\n","    a = point_head[1] - point_tail[1]\n","    b = point_tail[0] - point_head[0]\n","    c = a * point_head[0] + b * point_head[1]\n","    iter = 0\n","    flag = False\n","    sample_points = np.random.uniform(-1, 1, (N, 2))\n","    dummy_points = np.c_[np.ones(N), sample_points[:, 0], sample_points[:, 1]]\n","    coeffs = np.array([c, a, b])\n","\n","    # Create correct classification\n","    correct_classification = np.sign(np.dot(dummy_points, coeffs))\n","\n","    # Initailize weights\n","    w = np.zeros(3)\n","\n","    while flag == False:\n","      iter += 1\n","      # Perceptron\n","      h_x = np.sign(np.dot(dummy_points, w))\n","\n","      # Check for misclassification\n","      mis_idx = []\n","      for i in range(N):\n","        if h_x[i] != correct_classification[i]:\n","          mis_idx.append(i)\n","      # Exit the run if all points are correctly classified\n","      if len(mis_idx) == 0:\n","        flag = True\n","      else:\n","        idx = np.random.choice(mis_idx)\n","        w += correct_classification[idx] * dummy_points[idx]\n","    num_iter.append(iter)\n","\n","    # Generate another set of samples to test probability\n","    prob_samples = np.random.uniform(-1, 1, (10000, 2))\n","    prob_dummy_points = np.c_[prob_samples[:, 0], prob_samples[:, 1], np.ones(10000)]\n","    correct_probclass = np.sign(np.dot(prob_dummy_points, coeffs))\n","    actual_result = np.sign(np.dot(prob_dummy_points, w))\n","    prob = np.sum(correct_probclass != actual_result)/10000\n","    probs.append(prob)\n","\n","  # Output mean number of iterations and probability\n","  print(\"Mean number of iterations:\", np.mean(num_iter))\n","  print(\"Mean disagreement probability:\", np.mean(probs))\n"]},{"cell_type":"markdown","source":["From the simulations above, we can see that when $N = 10$, the number of iterations it takes to converge, is cloest to 15. And the probability for disagreement, is approximately $0.1$\n","\n","Hence, I choose **b** for question $7$, and **c** for question $8$.\n","\n","Also, for $N = 100$, the number of iterations it takes to converge, is cloest to 100. And the probability for disagreement, is approximately $0.01$.\n","\n","Hence, I choose **b** for question $9$, and **b** for question $10$.\n","\n"],"metadata":{"id":"DcqpBnPTNKw2"}}]}