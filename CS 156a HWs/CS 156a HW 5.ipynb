{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMSXNw9Ts2I2zBCYWJYc6Jy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Imports\n","import numpy as np"],"metadata":{"id":"nJYnRDlKQLgK","executionInfo":{"status":"ok","timestamp":1698526224729,"user_tz":420,"elapsed":182,"user":{"displayName":"Alfin Hou","userId":"02529736084419145803"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["##Question 1##\n","We want:\n","$$\\mathbb{E}_{D}(E_{in}(\\mathrm{x}_{lin}))>0.008$$\n","This is just:\n","$$\n","0.1^2 (1-\\frac{8+1}{N})>0.008 \\\\\n","1-\\frac{9}{N}>0.8 \\\\\n","\\frac{9}{N}<0.2 \\\\\n","N>45\n","$$\n","Hence, we get that we need $N>45$ to get the required result for expected $E_{in}$.  \n","Therefore, the answer for question $1$ is **[c]**."],"metadata":{"id":"D1HH_9-iFGxO"}},{"cell_type":"markdown","source":["##Question 2 - 3##\n","For question 2, we want $h(\\mathrm{x}) = +1$ with the following condition satisfied:\n","$$w̃_0+w̃_1x_{1}^2+w̃_2x_{2}^2>0$$\n","Now we can analyse the choices one by one:\n","- [a]:\n","Plug in [a], we will have $w̃_2x_{2}^2>-w̃_0$, which will result in a shape similar to a quadratic polynomial instead of a hyperbolic bound. Hence, [a] is not the correct answer.\n","- [b]:\n","Similar to [a], [b] will result in a bound that looks like a polynomial which is rotated by $90°$ (Based on the axes followed by $x_1,x_2$ in the plot provided). This is also not the required hyperbolic bound.\n","- [c]:\n","For [c], we will get the following inequality: $w̃_1x_{1}^2+w̃_2x_{2}^2 > -w̃_0$. Given $w̃_1,w̃_2>0$, this is just the whole set $\\mathbb{R}^2$ with $w̃_0>0$ or any point outside of the disc $w̃_1x_{1}^2+w̃_2x_{2}^2 \\leq -w̃_0$ with $w̃_0<0$. So this is not the required bound either.\n","- [d]:\n","The bound obtained here is equivalent to $-|w̃_1|x_{1}^2+|w̃_2|x_{2}^2 > -w̃_0$. In this case, if we choose $w̃_0>0$, then we will get the required hyperbolic bound we want.\n","- [e]:\n","The bound obtained here is equivalent to $|w̃_1|x_{1}^2-|w̃_2|x_{2}^2 > -w̃_0$. This is not the bound we want as now the region that returns $+1$ actually intersect the region that is expected to return $-1$, as this is just the region in [d] rotated by $90°$.  \n","\n","Therefore, the answer to question $2$ is **[d]**.\n","\n","For question 3, from lecture slide $9$, we know that in the transformed space, the VC dimension satisfies: $d_{VC}\\leq d̃+1$ and we have $d̃ = 14$.\n","Hence, the smallest value would just be $14+1=15$.\n","\n","Therefore, the answer to question $3$ is **[c]**."],"metadata":{"id":"5JWHkanVGd3Y"}},{"cell_type":"markdown","source":["## Question 4 - 7##\n","$$\\frac{\\partial E}{\\partial u} = \\frac{\\partial}{\\partial u}(ue^v - 2ve^{-u})^2 = 2(ue^v - 2ve^{-u})(e^v + 2ve^{-u})$$\n","Therefore, the answer to question $4$ is **[e]**."],"metadata":{"id":"ahfJOikOOYFw"}},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8M6bYEs9FB-S","executionInfo":{"status":"ok","timestamp":1698526224959,"user_tz":420,"elapsed":4,"user":{"displayName":"Alfin Hou","userId":"02529736084419145803"}},"outputId":"130e91d1-3bd0-4ccd-f55e-97b1ab783b5c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration: 10\n","Error: 1.2086833944220747e-15\n","[0.04473629 0.02395871]\n"]}],"source":["e = np.e\n","def error(u, v):\n","  return (u * (e ** v) - 2 * v * (e ** (-u))) ** 2\n","\n","starting_point = np.array([1,1])\n","err_iter = np.inf\n","iter = 0\n","learning_rate = 0.1\n","\n","w_curr = np.array([1,1])\n","\n","while err_iter >= 1e-14:\n","  w_prev = w_curr\n","  iter += 1\n","  if iter == 1:\n","    current_point = starting_point\n","  u = current_point[0]\n","  v = current_point[1]\n","  grad_E_in_u = 2 * (u * (e ** v) - 2 * v * (e ** (-u))) * ((e ** v) + 2 * v * (e ** (-u)))\n","  grad_E_in_v = 2 * (u * (e ** v) - 2 * v * (e ** (-u))) * (u * (e ** v) - 2 * (e ** (-u)))\n","  w_nabla = - learning_rate * np.array([grad_E_in_u, grad_E_in_v])\n","  w_curr = w_prev + w_nabla\n","  current_point = w_curr\n","  err_iter = error(current_point[0],current_point[1])\n","\n","print(\"Iteration:\", iter)\n","print(\"Error:\", err_iter)\n","print(current_point)\n"]},{"cell_type":"markdown","source":["For question 5 and 6, from the simulation, we know that $10$ is the iteration where the error dropped below given bound. And the point given is approximately $(0.045, 0.024)$\n","\n","Therefore, the answer to question 5 and 6 are **[d]** and **[e]**.\n"],"metadata":{"id":"cT8D7zXSerfB"}},{"cell_type":"code","source":["e = np.e\n","def error(u, v):\n","  return (u * (e ** v) - 2 * v * (e ** (-u))) ** 2\n","\n","err_iter = np.inf\n","iter = 0\n","learning_rate = 0.1\n","\n","w_curr = np.array([1,1])\n","\n","for i in range(15):\n","  w_prev = w_curr\n","  u = w_curr[0]\n","  v = w_curr[1]\n","  grad_E_in_u = 2 * (u * (e ** v) - 2 * v * (e ** (-u))) * ((e ** v) + 2 * v * (e ** (-u)))\n","  w_nabla = - learning_rate * np.array([grad_E_in_u, v])\n","  w_curr = w_prev + w_nabla\n","  err_iter = error(w_curr[0],w_curr[1])\n","  w_prev = w_curr\n","  u = w_curr[0]\n","  v = w_curr[1]\n","  grad_E_in_v = 2 * (u * (e ** v) - 2 * v * (e ** (-u))) * (u * (e ** v) - 2 * (e ** (-u)))\n","  w_nabla = - learning_rate * np.array([u, grad_E_in_v])\n","  w_curr = w_prev + w_nabla\n","  err_iter = error(w_curr[0],w_curr[1])\n","\n","print(err_iter)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CVxRsdG6fdWx","executionInfo":{"status":"ok","timestamp":1698526224959,"user_tz":420,"elapsed":3,"user":{"displayName":"Alfin Hou","userId":"02529736084419145803"}},"outputId":"da0991fb-fa24-4809-a9cd-be48caae4084"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["1.0511296429880623\n"]}]},{"cell_type":"markdown","source":["Hence, from the simulation, the answer to question 7 is **[a]**."],"metadata":{"id":"qtylnQPciEvt"}},{"cell_type":"markdown","source":["## Question 8 - 9##\n"],"metadata":{"id":"ANaScZksiLNi"}},{"cell_type":"code","source":["learning_rate = 0.01\n","N = 100\n","avg_iter = 0\n","avg_err = 0\n","for _ in range(100):\n","  point_head = np.random.uniform(-1, 1, 2)\n","  point_tail = np.random.uniform(-1, 1, 2)\n","  # Linear Coordinates\n","  a = point_head[1] - point_tail[1]\n","  b = point_tail[0] - point_head[0]\n","  c = a * point_head[0] + b * point_head[1]\n","  # Generate samples\n","  sample_points = np.random.uniform(-1, 1, (N, 2))\n","  dummy_points = np.c_[np.ones(N), sample_points[:, 0], sample_points[:, 1]]\n","  coeffs = np.array([c, a, b])\n","  y_output = np.sign(np.dot(dummy_points, coeffs))\n","  w = np.zeros(3)\n","  diff = np.inf\n","  iter = 0\n","  # Logistic Regression and SGD\n","  while diff >= 0.01:\n","    iter += 1\n","    w_prev = w\n","    # Get expected E_in for each sample point\n","    choice_idx = np.array([i for i in range(100)])\n","    np.random.shuffle(choice_idx)\n","    for i in range(N):\n","      selected_sample = dummy_points[choice_idx[i]]\n","      y = y_output[choice_idx[i]]\n","      E_in_grad = - (selected_sample * y)/(1 + np.exp(y * (np.dot(w.T, selected_sample))))\n","      w = w - learning_rate * E_in_grad\n","\n","    # Get the difference\n","    diff = np.linalg.norm(w - w_prev)\n","    new_sample = np.random.uniform(-1, 1, (1000, 2))\n","    new_dummy = np.c_[np.ones(1000), new_sample[:, 0], new_sample[:, 1]]\n","    new_y = np.sign(np.dot(new_dummy, coeffs))\n","    cross_entropy_err = 1/1000 * sum([np.log(1 + np.exp(-new_y[j] * np.dot(w.T, new_dummy[j]))) for j in range(1000)])\n","  avg_err += cross_entropy_err\n","  avg_iter += iter\n","\n","print(\"Average error:\",avg_err/100)\n","print(\"Average epoch:\", avg_iter/100)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hm5XCV_3ig3K","executionInfo":{"status":"ok","timestamp":1698526410157,"user_tz":420,"elapsed":185200,"user":{"displayName":"Alfin Hou","userId":"02529736084419145803"}},"outputId":"4eef4819-61b9-4fe8-a6a6-d500a0078b7a"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Average error: 0.10365729621878822\n","Average epoch: 343.92\n"]}]},{"cell_type":"markdown","source":["From the simulation result, we can conclude that the answer to question 8 and 9 are **[d]** and **[a]**."],"metadata":{"id":"3fxLCa_MEq1r"}},{"cell_type":"markdown","source":["## Question 10##\n","The error function should satisfy the property that, if $y_n$ and $w^Tx$ has the same sign (meaning the expected classification and the actual classification is the same). Then, the error should be $0$. Otherwise, it should be some positive number.  \n","Hence, [e] satisfies the requirement, because if $y_n$ and $w^Tx$ has the same sign, then $y_nw^Tx_n >0$. In this case, $-\\min(0, y_nw^Tx_n) = 0$, which is what we are expecting. And if $y_n, w^Tx_n$ have different signs, then one of them is positive and the other is negative, which means $y_nw^Tx_n < 0$. As a result, we will have $-\\min(0, y_nw^Tx_n) = -y_nw^Tx_n > 0$, which is the also the error that we expect.  \n","All other options can't give us zero error when $y_n, w^Tx_n$ get the same sign in general cases.  \n","\n","Therefore, the answer to question 10 is **[e]**."],"metadata":{"id":"wF4pWPaCy1qc"}}]}