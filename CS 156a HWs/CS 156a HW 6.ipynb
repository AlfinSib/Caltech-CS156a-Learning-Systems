{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1y9GFRBnEJIGm5AXSWUVZ3-hZTlOBeRZN","authorship_tag":"ABX9TyOtU9aG0hWBqRS+E5nQL8cL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Imports\n","import numpy as np\n","import scipy as sp"],"metadata":{"id":"3qE-DbHljWJS","executionInfo":{"status":"ok","timestamp":1699262668821,"user_tz":480,"elapsed":139,"user":{"displayName":"Alfin Hou","userId":"02529736084419145803"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["##Question 1##\n","Since deterministic noises are defined to be the part of the target function $f$ that the hypothesis set $\\mathscr{H}$ can't capture, we can expect that, due to the fact that $\\mathscr{H}' \\subseteq \\mathscr{H}$, more parts of $f$ can't be captured by $\\mathscr{H}'$ compared to $\\mathscr{H}$. Hence, the deterministic noise will increase.\n","\n","Therefore, the answer to question 1 is **[b]**.\n","\n"],"metadata":{"id":"VUSDApSAhIB7"}},{"cell_type":"markdown","source":["##Question 2 - 6##"],"metadata":{"id":"Ob_f_S8Vh7oK"}},{"cell_type":"code","execution_count":11,"metadata":{"id":"5N8TSU17hFDV","executionInfo":{"status":"ok","timestamp":1699262669308,"user_tz":480,"elapsed":352,"user":{"displayName":"Alfin Hou","userId":"02529736084419145803"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0216563d-56f3-4e10-bf99-87d0dba764e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["In-sample error (basic) 0.02857142857142857 Out-of-sample error (basic) 0.084\n","k = -3\n","In-sample error (decayed) 0.029525839138147505 Out-of-sample error (decayed) 0.08013361747934065\n","k = -2\n","In-sample error (decayed) 0.03725328025656029 Out-of-sample error (decayed) 0.08521545923591844\n","k = -1\n","In-sample error (decayed) 0.07247544093764077 Out-of-sample error (decayed) 0.06214656173126971\n","k = 0\n","In-sample error (decayed) 0.07203815960036912 Out-of-sample error (decayed) 0.10208534234405167\n","k = 1\n","In-sample error (decayed) 0.20012843656341833 Out-of-sample error (decayed) 0.14401798111887856\n","k = 2\n","In-sample error (decayed) 0.2833238906326495 Out-of-sample error (decayed) 0.23966534468857095\n","k = 3\n","In-sample error (decayed) 0.38510913211565906 Out-of-sample error (decayed) 0.4379152784961923\n"]}],"source":["# Load datasets\n","training_set = np.loadtxt(\"/content/drive/MyDrive/Colab Notebooks/CS 156a HWs/in.dta.txt\", dtype = np.float64)\n","test_set = np.loadtxt(\"/content/drive/MyDrive/Colab Notebooks/CS 156a HWs/out.dta.txt\")\n","\n","# Non-linear transform\n","x_1 = training_set[:,0]\n","x_2 = training_set[:,1]\n","trans_mat = np.c_[np.ones(len(x_1)), x_1, x_2, (x_1 ** 2), (x_2 ** 2), x_1 * x_2, abs(x_1 - x_2), abs(x_1 + x_2)]\n","x_test_1 = test_set[:,0]\n","x_test_2 = test_set[:,1]\n","trans_mat_test = np.c_[np.ones(len(x_test_1)), x_test_1, x_test_2, (x_test_1 ** 2), (x_test_2 ** 2), x_test_1 * x_test_2, abs(x_test_1 - x_test_2), abs(x_test_1 + x_test_2)]\n","\n","# Linear regression\n","y = training_set[:,2]\n","y_test = test_set[:,2]\n","\n","coeff_sol = np.dot(np.matmul(np.linalg.inv(np.matmul(trans_mat.T, trans_mat)), trans_mat.T), y)\n","\n","# Errors\n","in_sample = np.mean(np.not_equal(np.sign(np.matmul(trans_mat, coeff_sol)), y))\n","out_sample = np.mean(np.not_equal(np.sign(np.matmul(trans_mat_test, coeff_sol)), y_test))\n","print(\"In-sample error (basic)\", in_sample, \"Out-of-sample error (basic)\", out_sample)\n","\n","# Apply Weight Decay\n","k_list = [-3,-2,-1,0,1,2,3]\n","for k in k_list:\n","  lam = 10 ** k\n","  main_body = np.linalg.inv(np.matmul(trans_mat.T, trans_mat) + (lam * np.identity(8)))\n","  weights = np.matmul(np.matmul(main_body, trans_mat.T), y)\n","  in_sample = np.mean(np.not_equal(np.sign(np.matmul(trans_mat, weights)), y)) + (lam/len(y)) * np.dot(weights, weights)\n","  out_sample = np.mean(np.not_equal(np.sign(np.matmul(trans_mat_test, weights)), y_test)) + (lam/len(y_test)) * np.dot(weights, weights)\n","  print(\"k =\",k)\n","  print(\"In-sample error (decayed)\", in_sample, \"Out-of-sample error (decayed)\", out_sample)"]},{"cell_type":"markdown","source":["Therefore, from the simulation result, the answer to question 2 - 6 are:\n","2.   **[a]**\n","3.   **[d]**\n","4.   **[e]**\n","5.   **[d]**\n","6.   **[b]**\n"],"metadata":{"id":"4f347N5Uag0E"}},{"cell_type":"markdown","source":["## Question 7##\n","$\\mathscr{H}(10, 0, 3)$ is the subset of $\\mathscr{H}_{10}$ such that $w_q=0$ for all $q\\geq 3$. Hence, only $L_0(x), L_1(x)$ and $L_2(x)$ get non-zero coefficients. Hence, $\\mathscr{H}(10, 0, 3) = \\{h|h(x) = \\sum_{q = 0}^{2}w_qL_q(x) \\}= \\mathscr{H}_2$.  \n","For the same reason, $\\mathscr{H}(10, 0, 4)$ is the subset of $\\mathscr{H}_{10}$ such that only $L_0(x), L_1(x), L_2(x), L_3(x)$ get non-zero coefficients because for any $q\\geq 4$, $L_q(x)$ has zero coefficient. Hence, $\\mathscr{H}(10, 0, 4) = = \\{h|h(x) = \\sum_{q = 0}^{3}w_qL_q(x) \\}= \\mathscr{H}_3$.  \n","From what we derived above we can see that $\\mathscr{H}(10, 0, 3) \\subseteq \\mathscr{H}(10, 0, 4)$ because we have $\\mathscr{H}(10, 0, 3) = \\{h(x) = \\sum_{q = 0}^{3}w_qL_q(x)\\in \\mathscr{H}(10, 0, 4)|w_3=0\\}$  \n","Therefore, we get that $\\mathscr{H}(10, 0, 3) \\cap \\mathscr{H}(10, 0, 4) = \\mathscr{H}(10, 0, 3) = \\mathscr{H}_2$.\n","\n","Therefore, the answer to this question is **[c]**."],"metadata":{"id":"CNGp_XcKh4H4"}},{"cell_type":"markdown","source":["##Question 8 - 10##"],"metadata":{"id":"04CMxeK_o_Fm"}},{"cell_type":"markdown","source":["For question 8, firstly, for the forward part, from the $0$-th layer to the $1$-st layer, we have $0\\leq i \\leq 5=d^{(0)}$ and $1 \\leq j \\leq 3=d^{(1)}$ and from the $1$-st layer to the $2$-nd layer, we have $0\\leq i \\leq 3=d^{(1)}$ and $1 \\leq j \\leq 1=d^{(1)}$. Hence, for the operation $w_{ij}^{(l)}x_{i}^{(l-1)}$, the number it is performed is given by:\n","$$6\\times 3 + 4\\times 1 = 22$$\n","Since backward calculations is performed the same number of times as the forward part from layer $2$ to $1$ and we don't need to calculate $\\delta$ for the $0$-th layer. Thus, the number of operation $w_{ij}^{(l)}\\delta_{j}^{l}$ is\n","$$4\\times 1 = 4$$\n","And then, since the total number of $w_{ij}^{(l)}$ is equal to the number of forward steps, we know that it's also $22$.\n","Hence, the total number of operations is\n","$$22+22+4 = 48 \\approx 50$$\n","\n","Therefore, the answer to question 8 is **[e]**.\n","\n","For question 9, the minimum can be given by putting each hidden node in a unique layer, then the total number of layer is just as follows:\n","$$10 + 36 = 46$$\n","\n","Therefore, the answer to question 9 is **[a]**.\n","\n","For question 10, since the minimum is obtained by maximizing the number of layers, it's reasonable for us to assume that the maximum can be obtained using a relatively small number of layers. Below is a numerical approach that calculates the maximum number of weights given $1,2$ and $3$ layers:"],"metadata":{"id":"IpcovPKyvHyc"}},{"cell_type":"code","source":["one_layer_result = 10 * 35 + 36 * 1\n","two_layer_cases = np.array([[x,y] for x in range(1, 36) for y in range(1, 36) if x + y == 36])\n","two_layer_best_idx = np.argmax(10 * (two_layer_cases[:,0] - 1) + two_layer_cases[:,0] * (two_layer_cases[:,1] - 1) + two_layer_cases[:,1])\n","three_layer_cases = np.array([[x,y,z] for x in range(1, 36) for y in range(1, 36) for z in range(1, 36) if x + y + z == 36])\n","three_layer_best_idx = np.argmax(10 * (three_layer_cases[:,0] - 1) + three_layer_cases[:,0] * (three_layer_cases[:,1] - 1) + three_layer_cases[:,1] * (three_layer_cases[:,2] - 1) + (three_layer_cases[:,2]))\n","\n","two_layer_best = two_layer_cases[two_layer_best_idx]\n","best_two = (10 * (two_layer_best[0] - 1)) + (two_layer_best[0] * (two_layer_best[1] - 1)) + two_layer_best[1]\n","three_layer_best = three_layer_cases[three_layer_best_idx]\n","best_three = (10 * (three_layer_best[0] - 1)) + (three_layer_best[0] * (three_layer_best[1] - 1)) + (three_layer_best[1] * (three_layer_best[2] - 1)) + three_layer_best[2]\n","\n","print(\"Number of weights with one layer:\",one_layer_result)\n","print(\"Optimal node number and maximum number of weights with two layer:\", two_layer_best, best_two)\n","print(\"Optimal node number and maximum number of weights with three layer:\", three_layer_best, best_three)\n"],"metadata":{"id":"shLs-yI3c1Fg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699262743122,"user_tz":480,"elapsed":135,"user":{"displayName":"Alfin Hou","userId":"02529736084419145803"}},"outputId":"cd7d3fce-b67f-4e5d-b996-03abd9d21e22"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of weights with one layer: 386\n","Optimal node number and maximum number of weights with two layer: [22 14] 510\n","Optimal node number and maximum number of weights with three layer: [22 13  1] 475\n"]}]},{"cell_type":"markdown","source":["From the simulation, it's reasonable to conclude that the maximum is obtained with $2$ layers with the first layer having $22$ nodes and the second layer having $14$ nodes.\n","  \n","The resulting number of weights is $510$. Therefore, the answer to question 10 is **[e]**."],"metadata":{"id":"jpGeWlkK9Dqo"}}]}