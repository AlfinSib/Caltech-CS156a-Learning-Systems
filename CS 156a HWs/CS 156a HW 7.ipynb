{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"17Ju10QcYCY1OXRQ--CXKHsD6ARUWx3GT","authorship_tag":"ABX9TyPMQE3mwlGj1nrToOLg6IVa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Imports\n","import numpy as np\n","from scipy.optimize import minimize\n","from scipy.optimize import LinearConstraint\n","from scipy.optimize import Bounds\n","from sklearn import svm"],"metadata":{"id":"o79K8ni7R-uN","executionInfo":{"status":"ok","timestamp":1699851540220,"user_tz":480,"elapsed":140,"user":{"displayName":"Alfin Hou","userId":"02529736084419145803"}}},"execution_count":151,"outputs":[]},{"cell_type":"markdown","source":["##Question 1 - 5##\n"],"metadata":{"id":"rALTwkkKRLmp"}},{"cell_type":"code","source":["# Load data\n","training_set = np.loadtxt(\"/content/drive/MyDrive/Colab Notebooks/CS 156a HWs/in.dta.txt\", dtype = np.float64)\n","test_set = np.loadtxt(\"/content/drive/MyDrive/Colab Notebooks/CS 156a HWs/out.dta.txt\")\n","\n","# Split into different sets\n","D_train = training_set[0:25,:]\n","D_val = training_set[25:,:]\n","\n","\n","# Non-linenar transform\n","x_1 = D_train[:,0]\n","x_2 = D_train[:,1]\n","Z_train = np.c_[np.ones(25), x_1, x_2, (x_1 ** 2), (x_2 ** 2), x_1 * x_2, abs(x_1 - x_2), abs(x_1 + x_2)]\n","x_val_1 = D_val[:,0]\n","x_val_2 = D_val[:,1]\n","Z_val = np.c_[np.ones(10), x_val_1, x_val_2, (x_val_1 ** 2), (x_val_2 ** 2), x_val_1 * x_val_2, abs(x_val_1 - x_val_2), abs(x_val_1 + x_val_2)]\n","x_test_1 = test_set[:,0]\n","x_test_2 = test_set[:,1]\n","Z_test = np.c_[np.ones(len(x_test_1)), x_test_1, x_test_2, (x_test_1 ** 2), (x_test_2 ** 2), x_test_1 * x_test_2, abs(x_test_1 - x_test_2), abs(x_test_1 + x_test_2)]\n","\n","k_list = [4, 5, 6, 7, 8]\n","# Linear regression\n","for k in k_list:\n","  y = D_train[:,2]\n","  y_val = D_val[:,2]\n","  y_test = test_set[:,2]\n","  Z_train_sub = Z_train[:, 0:k]\n","  Z_val_sub = Z_val[:, 0:k]\n","  Z_test_sub = Z_test[:, 0:k]\n","  coeff_sol = np.dot(np.matmul(np.linalg.inv(np.matmul(Z_train_sub.T, Z_train_sub)), Z_train_sub.T), y)\n","\n","  # Errors\n","  E_in = np.mean(np.not_equal(np.sign(np.matmul(Z_train_sub, coeff_sol)), y))\n","  E_val = np.mean(np.not_equal(np.sign(np.matmul(Z_val_sub, coeff_sol)), y_val))\n","  E_out = np.mean(np.not_equal(np.sign(np.matmul(Z_test_sub, coeff_sol)), y_test))\n","  print(\"k =\",k - 1)\n","  print(\"In-sample error:\", E_in, \"Validation error:\", E_val, \"Out of sample error:\", E_out)\n","\n","# Swap training set and validation set\n","print(\"\\nTraining set and validation set swapped \\n\")\n","D_train, D_val = D_val, D_train\n","Z_train, Z_val = Z_val, Z_train\n","for k in k_list:\n","  y = D_train[:,2]\n","  y_val = D_val[:,2]\n","  y_test = test_set[:,2]\n","  Z_train_sub = Z_train[:, 0:k]\n","  Z_val_sub = Z_val[:, 0:k]\n","  Z_test_sub = Z_test[:, 0:k]\n","  coeff_sol = np.dot(np.matmul(np.linalg.inv(np.matmul(Z_train_sub.T, Z_train_sub)), Z_train_sub.T), y)\n","\n","  # Errors\n","  E_in = np.mean(np.not_equal(np.sign(np.matmul(Z_train_sub, coeff_sol)), y))\n","  E_val = np.mean(np.not_equal(np.sign(np.matmul(Z_val_sub, coeff_sol)), y_val))\n","  E_out = np.mean(np.not_equal(np.sign(np.matmul(Z_test_sub, coeff_sol)), y_test))\n","  print(\"k =\",k - 1)\n","  print(\"In-sample error:\", E_in, \"Validation error:\", E_val, \"Out of sample error:\", E_out)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fTih3QAOSBBq","executionInfo":{"status":"ok","timestamp":1699851540392,"user_tz":480,"elapsed":3,"user":{"displayName":"Alfin Hou","userId":"02529736084419145803"}},"outputId":"359108ab-a358-4697-a072-ca3224424580"},"execution_count":152,"outputs":[{"output_type":"stream","name":"stdout","text":["k = 3\n","In-sample error: 0.44 Validation error: 0.3 Out of sample error: 0.42\n","k = 4\n","In-sample error: 0.32 Validation error: 0.5 Out of sample error: 0.416\n","k = 5\n","In-sample error: 0.08 Validation error: 0.2 Out of sample error: 0.188\n","k = 6\n","In-sample error: 0.04 Validation error: 0.0 Out of sample error: 0.084\n","k = 7\n","In-sample error: 0.04 Validation error: 0.1 Out of sample error: 0.072\n","\n","Training set and validation set swapped \n","\n","k = 3\n","In-sample error: 0.4 Validation error: 0.28 Out of sample error: 0.396\n","k = 4\n","In-sample error: 0.3 Validation error: 0.36 Out of sample error: 0.388\n","k = 5\n","In-sample error: 0.2 Validation error: 0.2 Out of sample error: 0.284\n","k = 6\n","In-sample error: 0.0 Validation error: 0.08 Out of sample error: 0.192\n","k = 7\n","In-sample error: 0.0 Validation error: 0.12 Out of sample error: 0.196\n"]}]},{"cell_type":"markdown","source":["From the simulation results, the answer to questions 1 - 5 are:\n","\n","\n","1.   **[d]**\n","2.   **[e]**\n","3.   **[d]**\n","4.   **[d]**\n","5.   **[b]**\n","\n"],"metadata":{"id":"Z-xVFRhIbd4w"}},{"cell_type":"markdown","source":["##Question 6##\n","Since $e_1,e_2 \\sim \\mathrm{Unif}[0,1]$, we can directly get that:\n","$$\\mathbb{E}(e_1) = \\mathbb{E}(e_2) = \\frac{1}{2}(1-0) = 0.5$$\n","Then, for $e = \\min(e_1,e_2)$, consider $\\mathbb{P}(e>x),x\\in [0,1]$. We have the following equation:\n","$$\\mathbb{P}(e>x)=\\mathbb{P}(e_1>x, e_2>x)$$\n","Since $e_1,e_2$ are independent, we have $\\mathbb{P}(e_1>x, e_2>x) = \\mathbb{P}(e_1>x)\\mathbb{P}(e_2>x)=(1-\\mathbb{P}(e_1<x))(1 - \\mathbb{P}(e_2<x))=(1-x)^2$.  \n","Thus, we have $\\mathbb{P}(e>x) = (1-x)^2$ and as a result, we have: $$\\mathbb{P}(e<x) = 1-(1-x)^2=x(2-x)$$\n","So the PDF of $e$ is given by $f_e(x) = \\frac{d}{dx}x(2-x) = 2-2x$. Hence, the expectation of $e$ can be given as follows:\n","$$\\mathbb{E}(e) = \\int_{0}^{1} x(2 - 2x) dx=\\frac{1}{3}$$\n","This is closest to $0.4$.\n","\n","Therefore, the answer to question 6 is: **[d]**."],"metadata":{"id":"wAVu0kfycUR9"}},{"cell_type":"code","source":["# Self-testing\n","e = 0\n","for i in range(0,100000):\n","  e_1 = np.random.uniform()\n","  e_2 = np.random.uniform()\n","  e += np.min([e_1, e_2])\n","print(e/100000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AoRKzEwXoO_2","executionInfo":{"status":"ok","timestamp":1699851541715,"user_tz":480,"elapsed":1325,"user":{"displayName":"Alfin Hou","userId":"02529736084419145803"}},"outputId":"c710e70a-f8e9-400a-d02e-16a1915c1c4d"},"execution_count":153,"outputs":[{"output_type":"stream","name":"stdout","text":["0.3336642545015517\n"]}]},{"cell_type":"markdown","source":["##Question 7##\n","We can calculate the resulting functions and errors with each point removed:\n","- $(p,1)$ removed:\n","$h_0(x)=0$ and $h_1(x)=0$. In this case $e_0 = (1-0)^2=1=e_1$.\n","- $(1,0)$ removed:\n","$h_0(x)=0.5$ and $h_1(x)=\\frac{1}{p+1}x+\\frac{1}{p+1}$. In this case, $e_0=(1-0.5)^2=0.25$ and $e_1=(\\frac{2}{p+1}-0)^2 = \\frac{4}{(p+1)^2}$\n","- $(-1,0)$ removed:\n","$h_0(x)=0.5$ and $h_1(x)=\\frac{1}{p-1}x-\\frac{1}{p-1}$. In this case, $e_0=(1-0.5)^2=0.25$ and $e_1=(\\frac{-2}{p-1}-0)^2 = \\frac{4}{(p-1)^2}$\n","\n","Hence, we get that $\\mathbb{E}_{CV,h_0}=1+0.25+0.25=1.5$ and $\\mathbb{E}_{CV,h_1}=1 + \\frac{4}{(p-1)^2} + \\frac{4}{(p+1)^2}$. To make a tie, we need $\\mathbb{E}_{CV,h_1} = \\mathbb{E}_{CV,h_0}$, which can be simplified to the following:\n","$$\\frac{4}{(p-1)^2} + \\frac{4}{(p+1)^2} = 0.5$$\n","We will get the following equations:\n","\\begin{align*}\n","\\frac{4}{(p-1)^2} + \\frac{4}{(p+1)^2} &= 0.5\\\\\n","4((p+1)^2 + (p-1)^2) &= 0.5(p+1)^2(p-1)^2\\\\\n","16p^2+16 &= p^4 - 2p^2 + 1\\\\\n","p^4-18p^2-15&=0\n","\\end{align*}\n","Let $u = p^2$ and solve the equation $u^2-18u-15=0$, we get $u=p^2=9+4\\sqrt{6}$, which proves that $p=\\sqrt{9+4\\sqrt{6}}$.\n","\n","Therefore, the answer to question 7 is **[c]**."],"metadata":{"id":"KnVhNgG2kozi"}},{"cell_type":"markdown","source":["##Question 8 - 10##\n"],"metadata":{"id":"RPIici5tolBi"}},{"cell_type":"code","source":["# Store E_in and E_out in different experiments\n","\n","N_list = [10, 100]\n","def target(alpha,x,y):\n","  y = np.array([y])\n","  alpha_mat = np.array([alpha])\n","  y_part = np.matmul(y.T, y)\n","  x_part = np.matmul(x, x.T)\n","  return 1/2 * np.matmul(np.matmul(alpha_mat, x_part * y_part),alpha_mat.T) + np.dot(-1 * np.ones(len(alpha)), alpha)\n","\n","# Experiment (1000 runs)\n","num_support = 0\n","for N in N_list:\n","  prob_PLA = []\n","  prob_SVM = []\n","  print(\"N = \",N)\n","  for _ in range(1000):\n","    same_side = True\n","    while same_side == True:\n","      # Generate lines and samples\n","      point_head = np.random.uniform(-1, 1, 2)\n","      point_tail = np.random.uniform(-1, 1, 2)\n","      a = point_head[1] - point_tail[1]\n","      b = point_tail[0] - point_head[0]\n","      c = a * point_head[0] + b * point_head[1]\n","      sample_points = np.random.uniform(-1, 1, (N, 2))\n","      dummy_points = np.c_[sample_points[:, 0], sample_points[:, 1], np.ones(N)]\n","      coeffs = np.array([a, b, c])\n","\n","      # Create correct classification\n","      correct_classification = np.sign(np.dot(dummy_points, coeffs))\n","      if not(np.all(correct_classification == 1)) and not(np.all(correct_classification == -1)):\n","        same_side = False\n","\n","    # PLA\n","      # Initailize weights\n","    w = np.zeros(3)\n","    flag = False\n","    while flag == False:\n","      # Perceptron\n","      h_x = np.sign(np.dot(dummy_points, w))\n","\n","      # Check for misclassification\n","      mis_idx = []\n","      for i in range(N):\n","        if h_x[i] != correct_classification[i]:\n","          mis_idx.append(i)\n","      # Exit the run if all points are correctly classified\n","      if len(mis_idx) == 0:\n","        flag = True\n","      else:\n","        idx = np.random.choice(mis_idx)\n","        w += correct_classification[idx] * dummy_points[idx]\n","      # Generate another set of samples to test probability\n","    prob_samples = np.random.uniform(-1, 1, (10000, 2))\n","    prob_dummy_points = np.c_[prob_samples[:, 0], prob_samples[:, 1], np.ones(10000)]\n","    correct_probclass = np.sign(np.dot(prob_dummy_points, coeffs))\n","    actual_result = np.sign(np.dot(prob_dummy_points, w))\n","    prob = np.sum(correct_probclass != actual_result)/10000\n","    prob_PLA.append(prob)\n","\n","    # SVM\n","    svm_class = svm.SVC()\n","    # Setting larger \"C\" and linear kernel to mimic hard-margin SVM\n","    svm_class.C = 1e15\n","    svm_class.kernel = 'linear'\n","    svm_class.fit(dummy_points, correct_classification)\n","    result_SVM = svm_class.predict(prob_dummy_points)\n","    prob = np.sum(correct_probclass != result_SVM)/10000\n","    prob_SVM.append(prob)\n","    if N == 100:\n","      num_support += len(svm_class.support_vectors_)\n","\n","  prob_PLA = np.array(prob_PLA)\n","  prob_SVM = np.array(prob_SVM)\n","  print(\"Required percentage:\", np.sum(prob_PLA>prob_SVM)/1000)\n","print(\"Average number of support vectors (N = 100):\", num_support/1000)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MRO37jmip4zt","executionInfo":{"status":"ok","timestamp":1699851575613,"user_tz":480,"elapsed":33900,"user":{"displayName":"Alfin Hou","userId":"02529736084419145803"}},"outputId":"c313903b-9bcd-4a14-b9f8-b3ef0c793660"},"execution_count":154,"outputs":[{"output_type":"stream","name":"stdout","text":["N =  10\n","Required percentage: 0.606\n","N =  100\n","Required percentage: 0.655\n","Average number of support vectors (N = 100): 2.999\n"]}]},{"cell_type":"markdown","source":["Thus, from the simulation above, we know that the answers to question $8-10$ are:\n","\n","\n","8.   **[c]**\n","9.   **[d]**\n","10.  **[b]**\n","\n"],"metadata":{"id":"Cc2dqh4wtWA1"}}]}